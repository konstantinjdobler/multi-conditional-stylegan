# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

"""Project given image to the latent space of pretrained network pickle."""

from interpolation import mapping
import functools
import os
from time import time
from training.training_loop import save_image_grid
import pathlib
from generate_grid import condition_list, get_fuzzy_match, parse_conditions, ModifiedPath
import json

import click
import numpy as np
import torch
import torch.nn.functional as F
from itertools import combinations

import dnnlib
import legacy
from training.dataset import ImageFolderDataset


#----------------------------------------------------------------------------

def euclidian_distance(d1, d2=None):
    d1 = np.array(d1)
    d2 =  0 if d2 is None else np.array(d2)

    dist = ((d1 - d2)**2).sum(axis=1)**(0.5)
    return dist

def choose_mapping(labels, seed, num, **mapping_kwargs):
    chosen_idx = np.random.RandomState(seed).choice(len(labels), num)
    condition = labels[chosen_idx]
    ws = mapping(seed=seed+1, condition=condition, num=num, **mapping_kwargs)
    ws = ws[:,0,:]
    return ws

def analyze_intersection(labels, idx, c1, c2):
    # Filter labels
    x1 = [l for l in labels if l[idx] == c1]
    x2 = [l for l in labels if l[idx] == c2]

    # Remove idx condition to compare other conditions
    d_x1 = np.delete(x1, idx, axis=1).tolist()
    d_x2 = np.delete(x2, idx, axis=1).tolist()

    # Unique condition combinations
    u_x1 = set(map(tuple, d_x1))
    u_x2 = set(map(tuple, d_x2))
    u = u_x1.intersection(u_x2)
    
    print(f"c1: {len(u_x1)}, c2: {len(u_x2)}, In common: {len(u)}")

    # Find combinations that also occur with the other condition
    f_x1 = [x for x in d_x1 if tuple(x) in u_x2]
    f_x2 = [x for x in d_x2 if tuple(x) in u_x1]

    print(f"c1: {len(x1)}, c2: {len(x2)}, c1->c2: {len(f_x1)}, c2->c1: {len(f_x2)}")
    
    # Unique condition combinations of c1 that also occur with c2
    r_x1 = [x for x in x1 if tuple(np.delete(x, idx)) in u_x2]
    r_x1 = np.unique(r_x1, axis=0).tolist()

    return r_x1



#----------------------------------------------------------------------------

@click.command()
@click.option('--network', 'network_pkl', help='Network pickle filename', required=True)
@click.option('--num-samples',            help='Number of samples to draw', type=int, default=1000, show_default=True)
@click.option('--seed',                   help='Random seed', type=int, default=303, show_default=True)
@click.option('--outdir',                 help='Where to save the output images', required=True, metavar='DIR')
@click.option('--massive-multi-domain-conditions', '--mmdc', help="Specify the directory where the MMDC annotations (generated by the `create_label_json.py` script) are stored."
              "If this option is not provided, MMDC is not used. If provided without a value, a default value is used.", is_flag=False, flag_value="./annotations/emotions-artist-style-genre", type=ModifiedPath(file_okay=False, exists=True, path_type=pathlib.Path), default=False, is_eager=True)
@click.option('--conditions', help='Condition from', type=condition_list)
def run_vector_arithmetic(
    network_pkl: str,
    outdir: str,
    seed: int,
    num_samples: int,
    massive_multi_domain_conditions: ModifiedPath,
    conditions,
):
    """Project given image to the latent space of pretrained network pickle.

    Examples:

    \b
    python projector.py --outdir=out --target=~/mytargetimg.png \\
        --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl
    """
    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    assert network_pkl
    print('Loading networks from "%s"...' % network_pkl)
    with dnnlib.util.open_url(network_pkl) as f:
        G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore

    if not torch.cuda.is_available():
        # https://github.com/NVlabs/stylegan2-ada-pytorch/pull/121/files
        G.forward = functools.partial(G.forward, force_fp32=True)
        G.synthesis.forward = functools.partial(G.synthesis.forward, force_fp32=True)

    # <condition_key>=<from_cond>,<to_cond>
    assert len(conditions) == 2
    conditions = np.array(conditions, dtype=object)
    idx_1 = np.argwhere(conditions[0])[0]
    idx_2 = np.argwhere(conditions[1])[0]
    assert len(idx_1) == 1 and idx_1 == idx_2

    idx = idx_1[0]
    from_condition, to_condition = tuple(conditions[:,idx])

    with open(massive_multi_domain_conditions / "prepared_dataset.json") as f:
        data = json.load(f)
        original_labels = data["labels"]
        label_shapes = data["shapes"]
        labels = [l[1] for l in original_labels]
    
        condition_order = data["condition_order"]

    def original_labels(idx):
        return list([indices[i][idx] for i, idx in enumerate(idx)])
    
    indices = []
    for c in condition_order:
        with open(massive_multi_domain_conditions / f"{c}_idx.json") as f:
            index = list(json.load(f).keys())
            indices.append(index)

    
    unique = analyze_intersection(labels, idx, from_condition, to_condition)
    original = [original_labels(l) for l in unique]
    result = np.delete(original, idx, axis=1) if len(unique) > 1 else []
    print(result)

    filtered_from_labels = [l for l in labels if l[idx] == from_condition]
    filtered_from_labels = np.array(filtered_from_labels)
    print("#Labels", len(filtered_from_labels))
    to_labels = filtered_from_labels.copy()
    to_labels[:,idx] = to_condition
    
    embedded_from_labels = ImageFolderDataset.transform_multidomain_conditions(filtered_from_labels.tolist(), label_shapes)
    embedded_to_labels = ImageFolderDataset.transform_multidomain_conditions(to_labels.tolist(), label_shapes)

    d_avgs = []
    for n in [num_samples]:
        mapping_kwargs = dict(G=G, seed=seed, num=n, device=device)
        from_ws = choose_mapping(embedded_from_labels, **mapping_kwargs)
        to_ws = choose_mapping(embedded_to_labels, **mapping_kwargs)

        d = to_ws - from_ws

        d_avg = d.mean(axis=0)
        d_avgs.append(d_avg)

    for n in map(lambda x: 10**x, range(5)):
        _seed = int(time())
        mapping_kwargs = dict(G=G, seed=_seed, num=n, device=device)

        from_ws = choose_mapping(embedded_from_labels, **mapping_kwargs)
        to_ws = choose_mapping(embedded_to_labels, **mapping_kwargs)
        dest_ws = from_ws + d_avg

        dist_1 = euclidian_distance(dest_ws, to_ws).mean()
        x = ((dist_1**2)/len(d_avg))**(0.5)

        dist_2 = euclidian_distance(to_ws, from_ws).mean()
        dist_3 = euclidian_distance([d_avg]).mean()
        print(n, dist_1, dist_2, dist_3)
        print(x, min(d_avg), max(d_avg))

    # d_avgs = np.array(d_avgs)
    # print(d_avgs.shape)
    # np.savez(f'd_avg.npz', d=d_avgs)

    # pr = np.array(list(combinations(d, 2)))
    # d1 = torch.from_numpy(pr[:,0,:]).squeeze(1)
    # d2 = torch.from_numpy(pr[:,1,:]).squeeze(1)
    
    # sim = torch.nn.functional.cosine_similarity(d1, d2).numpy()
    # print(sim[:10], sim.mean(), sim.std())
    # eucl = (((d1 - d2)**2).sum(dim=1)**(0.5)).numpy()
    # print(eucl[:10], eucl.mean(), eucl.std())
    # l = np.linalg.norm(d, axis=1)
    # print(l[:10], l.mean(), l.std())

    d_avgs = np.vstack([d_avg] * G.num_ws)
    x = ((dist_1**2)/len(d_avg))**(0.5) * 3
    x = np.array([x] * len(d_avg))

    os.makedirs(outdir, exist_ok=True)
    timestamp = int(time())
    imgs_outdir = pathlib.Path(outdir) / str(timestamp)
    os.makedirs(imgs_outdir)
    print(f"Images are saved in {imgs_outdir}")
    
    num_imgs = 3
    n = 5
    grid_imgs = []
    for i in range(num_imgs):
        z = np.random.RandomState(seed+100+i).randn(1, G.z_dim)
        z = np.vstack([z] * 2)
        z = torch.from_numpy(z).to(device)

        chosen_idx = np.random.RandomState(seed+101+i).choice(len(filtered_from_labels), 1)[0]
        chosen_label = filtered_from_labels[chosen_idx]
        print(f"Row {i}: {original_labels(chosen_label.tolist())}")
        to_label = chosen_label.copy()
        to_label[idx] = to_condition
        conditions = np.array([chosen_label, to_label])
        c = ImageFolderDataset.transform_multidomain_conditions(conditions.tolist(), label_shapes, verbose=False)
        c = torch.from_numpy(c).to(device)
        
        normal_imgs = G(z, c, noise_mode="const").cpu().numpy()
        ws_from = G.mapping(z[0:1], c[0:1])
        ws_from = ws_from.squeeze(0).cpu().numpy()
        
        ws_d = ws_from + d_avgs
        ws_d = torch.from_numpy(ws_d).to(device)
        ws_d = ws_d.unsqueeze(0)
        d_img = G.synthesis(ws_d, noise_mode="const").cpu().numpy()

        
        ws_to = G.mapping(z[1:2], c[1:2])
        ws_to = np.vstack([ws_to] * 5)

        r = np.random.RandomState(seed+102+i).rand(n, len(d_avg))
        alt = np.where(r > 0.5, 1, -1)
        alt_x = alt * x
        alt_x = np.expand_dims(alt_x, axis=1)
        s_to = ws_to + alt_x
        s_to = torch.from_numpy(s_to).to(device)

        alt_img = G.synthesis(s_to, noise_mode="const").cpu().numpy()

        imgs = np.concatenate((normal_imgs, d_img, alt_img))
        grid_imgs.append(imgs)
    
    grid_imgs = np.concatenate(grid_imgs)
    save_image_grid(grid_imgs, imgs_outdir / f'grid-seed{seed:04d}.jpg', drange=[-1,1], grid_size=(3+n,num_imgs))

#----------------------------------------------------------------------------

if __name__ == "__main__":
    run_vector_arithmetic() # pylint: disable=no-value-for-parameter

#----------------------------------------------------------------------------
